#-----------------------------------------------------------------------------
# Í≤ΩÍ≥†Î¨∏ Ïïà Îú®Í≤å Ï≤òÎ¶¨
#-----------------------------------------------------------------------------
import logging
logging.getLogger().setLevel(logging.ERROR)
logging.getLogger().setLevel(logging.CRITICAL)

import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

#-----------------------------------------------------------------------------

import os
import json
import torch
import soundfile as sf
import librosa
import numpy as np
from datetime import datetime

from src.model.modeling_enh import VoiceFilter
from src.model.configuration_voicefilter import VoiceFilterConfig

#-----------------------------------------------------------------------------
# 0. GPU Ï≤¥ÌÅ¨
#-----------------------------------------------------------------------------
use_gpu = torch.cuda.is_available()
device = torch.device("cuda" if use_gpu else "cpu")
print(device)

#-----------------------------------------------------------------------------
# 1. HF inference-style WAV loader
#-----------------------------------------------------------------------------
def load_wav_hf(path, target_sr=16000):
    """HF inferenceÏôÄ ÎèôÏùºÌïú Î∞©ÏãùÏúºÎ°ú wav Î°úÎìú."""
    try:
        wav, sr = sf.read(path)
        if wav.ndim > 1:
            wav = wav.mean(axis=1)
        if sr != target_sr:
            wav = librosa.resample(wav, orig_sr=sr, target_sr=target_sr)
        return wav.astype(np.float32)
    except:
        wav, sr = librosa.load(path, sr=target_sr, mono=True)
        return wav.astype(np.float32)


#-----------------------------------------------------------------------------
# 2. Padding (HFÏôÄ ÎèôÏùº)
#-----------------------------------------------------------------------------
def pad_to_chunk(wav, chunk_size):
    rem = len(wav) % chunk_size
    if rem == 0:
        return wav
    pad_len = chunk_size - rem
    return np.concatenate([wav, np.zeros(pad_len, dtype=np.float32)])


#-----------------------------------------------------------------------------
# 3. HF-style xvector embedding
#-----------------------------------------------------------------------------
def cal_xvector_sincnet_embedding(xvector_model, ref_wav, sr=16000, max_length=5):
    chunk_len = max_length * sr
    chunks = []

    for i in range(0, len(ref_wav), chunk_len):
        w = ref_wav[i:i + chunk_len]
        if len(w) < chunk_len:
            w = np.concatenate([w, np.zeros(chunk_len - len(w))])
        chunks.append(w)

    chunks = np.array(chunks, dtype=np.float32)
    chunks = torch.from_numpy(chunks).unsqueeze(1)
    if use_gpu:
        chunks = chunks.cuda()

    with torch.no_grad():
        emb = xvector_model(chunks)

    return emb.mean(dim=0).cpu()


#-----------------------------------------------------------------------------
# 4. Î°úÏª¨ Î™®Îç∏ Î°úÎçî (HF from_pretrained ÏôÑÎ≤Ω Ïû¨ÌòÑ)
#-----------------------------------------------------------------------------
def load_voicefilter_model_local():
    config_path = "pretrained/config.json"
    ckpt_path   = "/root/VoiceFiltering_finetuning/FineTuning/2025-12-04_20-12-28/checkpoints/best_model_val.bin"
    # ckpt_path   = "pretrained/pytorch_model.bin"

    # config Î°úÎìú
    config = VoiceFilterConfig.from_pretrained(config_path)

    # Î™®Îç∏ ÏÉùÏÑ±
    model = VoiceFilter(config)

    # Í∞ÄÏ§ëÏπò Î°úÎìú
    state = torch.load(ckpt_path, map_location="cpu")
    missing, unexpected = model.load_state_dict(state, strict=False)

    # print("[Local Model Load] Missing:", missing)
    # print("[Local Model Load] Unexpected:", unexpected)
    print("\n=== Local ConVoiFilter Loaded ===")

    # inference mode
    model.eval()

    # xvector freeze
    model.xvector_model.eval()
    for p in model.xvector_model.parameters():
        p.requires_grad = False

    return model


#-----------------------------------------------------------------------------
# 5. Inference wrapper (do_enh Í∑∏ÎåÄÎ°ú)
#-----------------------------------------------------------------------------
def enhance_audio(model, mix_wav, ref_wav, sr=16000):
    chunk_size = model.wav_chunk_size

    mix_wav = pad_to_chunk(mix_wav, chunk_size)
    ref_wav = pad_to_chunk(ref_wav, chunk_size)

    mix_tensor = torch.tensor(mix_wav, dtype=torch.float32).to(device)
    ref_tensor = torch.tensor(ref_wav, dtype=torch.float32).to(device)

    # embedding
    with torch.no_grad():
        spk_emb = cal_xvector_sincnet_embedding(model.xvector_model,
                                                ref_tensor.cpu().numpy(),
                                                sr=sr)
        spk_emb = spk_emb.to(device)

    # enhancement
    with torch.no_grad():
        enhanced = model.do_enh(mix_tensor, spk_emb)

    return enhanced.cpu().numpy()

def calc_inference_loss(model, mix_wav, clean_wav, spk_emb, sr=16000):
    """
    Train/ValÏóêÏÑú Ïì∞Îäî loss Í≥ÑÏÇ∞ Î∞©Ïãù Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©.
    """
    model.eval()
    mix_wav = pad_to_chunk(mix_wav, model.wav_chunk_size)
    clean_wav = pad_to_chunk(clean_wav, model.wav_chunk_size)

    # tensor shapes ÎßûÏ∂îÍ∏∞
    mix_t = torch.tensor(mix_wav).float().unsqueeze(0).to(device)     # [1, T]
    clean_t = torch.tensor(clean_wav).float().unsqueeze(0).to(device) # [1, T]
    lengths = torch.tensor([mix_t.size(-1)]).to(device)

    if spk_emb.dim() == 1:
        spk_emb = spk_emb.unsqueeze(0).to(device)   # [1, D]
    else:
        spk_emb = spk_emb.to(device)

    with torch.no_grad():
        out = model(
            speech=mix_t,
            speech_lengths=lengths,
            target_speech=clean_t,
            target_spk_embedding=spk_emb,
        )

    return out.loss.item()


#-----------------------------------------------------------------------------
# 6. Main ‚Äì Test only
#-----------------------------------------------------------------------------
if __name__ == "__main__":

    # 1. Î™®Îç∏ Î°úÎìú
    model = load_voicefilter_model_local().to(device)

    # 2. ÌÖåÏä§Ìä∏ ÌååÏùº Î°úÎìú
    mix_path = "/root/VoiceFiltering_finetuning/Dataset/Test_Dataset/Mix/mix_648.wav" # ÎÖ∏Ïù¥ÏßÄ Îç∞Ïù¥ÌÑ∞
    ref_path = "/root/VoiceFiltering_finetuning/Dataset/Test_Dataset/Target/enrollment_648.wav" # ÌÉÄÍ≤ü Îç∞Ïù¥ÌÑ∞
    clean_path = "/root/VoiceFiltering_finetuning/Dataset/Test_Dataset/Clean/target_648.wav" # Ï†ïÎãµ Îç∞Ïù¥ÌÑ∞

    mix_wav = load_wav_hf(mix_path)
    ref_wav = load_wav_hf(ref_path)

    # 3. ÏùåÏÑ± Ìñ•ÏÉÅ Ïã§Ìñâ
    enhanced_audio = enhance_audio(model, mix_wav, ref_wav, sr=16000)

    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    result_dir = f"./results/{timestamp}"
    os.makedirs(result_dir, exist_ok=True)

    out_path = os.path.join(result_dir, "enhanced_output.wav")
    sf.write(out_path, enhanced_audio, 16000)

    print(f"üéâ Done! Enhanced audio saved at:\n‚û°  {out_path}\n")

    mix_wav = load_wav_hf(mix_path)
    clean_wav = load_wav_hf(clean_path)
    ref_wav = load_wav_hf(ref_path)

    # speaker embedding
    with torch.no_grad():
        spk_emb = cal_xvector_sincnet_embedding(model.xvector_model,
                                                ref_wav, sr=16000).to(device)

    # === Loss (Train/ValÍ≥º ÎèôÏùºÌïú Î∞©Ïãù) ===
    test_loss = calc_inference_loss(model,
                                    mix_wav,
                                    clean_wav,
                                    spk_emb,
                                    sr=16000)

    print(f"[Inference SI-SNR Loss] {test_loss:.4f}")
